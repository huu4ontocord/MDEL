dataset_name: Multi-Domain-Expert-Layers/arxiv
model_name_or_path: EleutherAI/pythia-1b-deduped
output_dir: "ckpts/pythia-1b-deduped/uspto/layer_9,10,11,12,13"
per_device_train_batch_size: 1
per_device_eval_batch_size: 8
preprocessing_num_workers: 32
learning_rate: 0.0001
block_size: 512
num_train_epochs: 1
gradient_accumulation_steps: 8
do_train: true
do_eval: true
evaluation_strategy: steps
eval_steps: 200
overwrite_output_dir: true
logging_steps: 20
max_steps: 1000
push_to_hub: true
push_to_hub_model_id: expert-uspto
push_to_hub_organization: Multi-Domain-Expert-Layers
wandb_entity: "ontocord"
wandb_project: "layer-experts"